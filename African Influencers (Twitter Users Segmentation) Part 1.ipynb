{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## African influencers: Twitter users segmentation \n",
    "### The goal is to Identify influencers rank position from Twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Web scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "import fire\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from website for 100 most influential Twitter users in Africa\n",
    "page_url = 'https://africafreak.com/100-most-influential-twitter-users-in-africa'\n",
    "page = requests.get(page_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "data = soup.find(id='content-area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering to find the name and username of the 100 most influential Twitter users in Africa\n",
    "influencer_name = data.find_all('h2')\n",
    "africa_influencers = []\n",
    "for influencer in influencer_name:\n",
    "    influencer = influencer.text\n",
    "    africa_influencers.extend([influencer])\n",
    "    \n",
    "africa_influencers = [word.replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\") for word in africa_influencers]\n",
    "africa_influencers = africa_influencers[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#into a dataframe\n",
    "africa_influencers = [influencer.split('@') for influencer in africa_influencers]\n",
    "africa_influencers_data = pd.DataFrame(africa_influencers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small Clean up\n",
    "def remove(list): \n",
    "    pattern = '[0-9]'\n",
    "    list = [re.sub(pattern, '', i) for i in list] \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the clean up\n",
    "africa_influencers_data[0] = remove(africa_influencers_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "africa_influencers_data = africa_influencers_data.rename(columns={0:'Name',1:'Username'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty column\n",
    "africa_influencers_data = africa_influencers_data.drop(columns = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from website for African leaders respond to coronavirus… on Twitter\n",
    "another_page_url = 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "another_page = requests.get(another_page_url)\n",
    "another_soup = BeautifulSoup(another_page.content, 'html.parser')\n",
    "more_data = another_soup.find(id=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering to find the name and username for African leaders who responded to coronavirus… on Twitter\n",
    "leader_name = more_data.find_all('blockquote')\n",
    "splitting = []\n",
    "for leader in leader_name:\n",
    "    leader = leader.text\n",
    "    leader = leader.split('—')\n",
    "    splitting.extend([leader[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_leaders = []\n",
    "for leader in splitting:\n",
    "    leader = leader.split('March')\n",
    "    african_leaders.extend([leader[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small Clean up\n",
    "def special_remove(list): \n",
    "    list = [word.replace(\"(\", \"\").replace(\")\", \"\")for word in list]\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cleanup\n",
    "african_leaders = special_remove(african_leaders)\n",
    "#the clean got rid of numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to dataframe\n",
    "african_leaders = [leader.split('@') for leader in african_leaders]\n",
    "african_leaders_data = pd.DataFrame(african_leaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_leaders_data = african_leaders_data.rename(columns={0:'Name',1:'Username'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "leader_usernames = list(african_leaders_data['Username'])\n",
    "influencer_usernames = list(africa_influencers_data['Username'])\n",
    "leader_usernames = [username.replace(\" \", \"\") for username in leader_usernames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(leader_usernames).to_csv('data/leader_usernames.csv')\n",
    "pd.DataFrame(influencer_usernames).to_csv('data/influencer_usernames.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use twitter data to rank the top leaders and influencers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import twitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smwik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smwik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the necessary methods from tweepy library  \n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "\n",
    "#sentiment analysis package\n",
    "from textblob import TextBlob\n",
    "\n",
    "#general text pre-processor\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#tweet pre-processor \n",
    "import preprocessor as p\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from pathlib import Path  # Python 3.6+ only\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables that contains the user credentials to access Twitter API \n",
    "consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "#This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting the reach_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find user object to get user data\n",
    "african_leaders_dataset = []\n",
    "for username in leader_usernames:\n",
    "    try:\n",
    "        search = api.search_users(username) \n",
    "        for user in search: \n",
    "            if user.screen_name == username:\n",
    "                africa_leaders_data = []\n",
    "                africa_leaders_data.extend([user.name,user.screen_name,user.followers_count,user.friends_count,user.statuses_count])\n",
    "                african_leaders_dataset.extend([africa_leaders_data])\n",
    "                \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_influencers_dataset = []\n",
    "for username in influencer_usernames:\n",
    "    try:\n",
    "        search = api.search_users(username) \n",
    "        for user in search: \n",
    "            if user.screen_name == username:\n",
    "                africa_influencers_data = []\n",
    "                africa_influencers_data.extend([user.name,user.screen_name,user.followers_count,user.friends_count,user.statuses_count])\n",
    "                african_influencers_dataset.extend([africa_influencers_data])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_influencers_dataframe = pd.DataFrame(african_influencers_dataset,columns = ['Name','Username','Number of Followers','Number of Following','Number of Retweets and Tweets'])\n",
    "african_leaders_dataframe = pd.DataFrame(african_leaders_dataset,columns = ['Name','Username','Number of Followers','Number of Following','Number of Retweets and Tweets'])\n",
    "\n",
    "#getting their reach_score = followers-following\n",
    "african_influencers_dataframe['reach_score']= african_influencers_dataframe['Number of Followers'] - african_influencers_dataframe['Number of Following']\n",
    "african_leaders_dataframe['reach_score']= african_leaders_dataframe['Number of Followers'] - african_leaders_dataframe['Number of Following']\n",
    "\n",
    "african_leaders_dataframe.to_csv('data/leaders_reach.csv', index = False, header=True)\n",
    "african_influencers_dataframe.to_csv('data/influencers_reach.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_reach_influencers = african_influencers_dataframe.sort_values('reach_score',ascending=False).head(10)\n",
    "top_reach_influencers.to_csv ('data/top_reach_influencers.csv', index = False, header=True)\n",
    "\n",
    "top_reach_leaders = african_leaders_dataframe.sort_values('reach_score',ascending=False).head(10)\n",
    "top_reach_leaders.to_csv ('data/top_reach_leaders.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetsearch():\n",
    "    '''\n",
    "    This is a basic class to search and download twitter data.\n",
    "    You can build up on it to extend the functionalities for more \n",
    "    sophisticated analysis\n",
    "    '''\n",
    "    def __init__(self, cols=None,auth=None):\n",
    "        #\n",
    "        if not cols is None:\n",
    "            self.cols = cols\n",
    "        else:\n",
    "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
    "                    'sentiment','polarity','subjectivity', 'lang',\n",
    "                    'favorite_count', 'retweet_count', 'original_author',   \n",
    "                    'possibly_sensitive', 'hashtags',\n",
    "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
    "            \n",
    "        if auth is None:\n",
    "            \n",
    "            #Variables that contains the user credentials to access Twitter API \n",
    "            consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "            consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "            access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "            access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "            \n",
    "\n",
    "\n",
    "            #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "            auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "            auth.set_access_token(access_token, access_token_secret)\n",
    "            \n",
    "\n",
    "        #            \n",
    "        self.auth = auth\n",
    "        self.api = tweepy.API(auth,wait_on_rate_limit=True) \n",
    "        self.filtered_tweet = ''\n",
    "            \n",
    "\n",
    "    def clean_tweets(self, twitter_text):\n",
    "\n",
    "        #use pre processor\n",
    "        tweet = p.clean(twitter_text)\n",
    "\n",
    "         #HappyEmoticons\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        #Emoji patterns\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                 u\"\\U00002702-\\U000027B0\"\n",
    "                 u\"\\U000024C2-\\U0001F251\"\n",
    "                 \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        #combine sad and happy emoticons\n",
    "        emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(tweet)\n",
    "        #after tweepy preprocessing the colon symbol left remain after      \n",
    "        #removing mentions\n",
    "        tweet = re.sub(r':', '', tweet)\n",
    "        tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "\n",
    "        #replace consecutive non-ASCII characters with a space\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        #remove emojis from tweet\n",
    "        tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "        #filter using NLTK library append it to a string\n",
    "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        #looping through conditions\n",
    "        filtered_tweet = []    \n",
    "        for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "                filtered_tweet.append(w)\n",
    "\n",
    "        return ' '.join(filtered_tweet)            \n",
    "\n",
    "    def get_tweets(self, keyword, csvfile=None):\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "\n",
    "        #page attribute in tweepy.cursor and iteration\n",
    "        for page in tweepy.Cursor(self.api.search, q=keyword,count=20, include_rts=False,tweet_mode='extended' , since = '2020-05-01').pages(5):\n",
    "\n",
    "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
    "            for status in page:\n",
    "                \n",
    "                new_entry = []\n",
    "                status = status._json\n",
    "                \n",
    "                #if this tweet is a retweet update retweet count\n",
    "                if status['created_at'] in df['created_at'].values:\n",
    "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                    #\n",
    "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                    if cond1 or cond2:\n",
    "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "\n",
    "                #calculate sentiment\n",
    "                filtered_tweet = self.clean_tweets(status['full_text'])\n",
    "                blob = TextBlob(filtered_tweet)\n",
    "                Sentiment = blob.sentiment     \n",
    "                polarity = Sentiment.polarity\n",
    "                subjectivity = Sentiment.subjectivity\n",
    "\n",
    "                new_entry += [status['id'], status['created_at'],\n",
    "                              status['source'], status['full_text'], filtered_tweet, \n",
    "                              Sentiment,polarity,subjectivity, status['lang'],\n",
    "                              status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "                new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "                try:\n",
    "                    is_sensitive = status['possibly_sensitive']\n",
    "                except KeyError:\n",
    "                    is_sensitive = None\n",
    "\n",
    "                new_entry.append(is_sensitive)\n",
    "\n",
    "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "                new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "                #\n",
    "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "                new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "                try:\n",
    "                    xyz = status['place']['bounding_box']['coordinates']\n",
    "                    coordinates = [coord for loc in xyz for coord in loc]\n",
    "                except TypeError:\n",
    "                    coordinates = None\n",
    "                #\n",
    "                new_entry.append(coordinates)\n",
    "\n",
    "                try:\n",
    "                    location = status['user']['location']\n",
    "                except TypeError:\n",
    "                    location = ''\n",
    "                #\n",
    "                new_entry.append(location)\n",
    "\n",
    "                #now append a row to the dataframe\n",
    "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
    "                df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        #\n",
    "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "        df = df.drop('id',axis=1)\n",
    "        \n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile,mode='a', index=True, encoding=\"utf-8\")\n",
    "            \n",
    "\n",
    "        return df \n",
    "    \n",
    "    def get_tweets_again(self, username, csvfile=None):\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=self.cols)\n",
    "        \n",
    "\n",
    "        #page attribute in tweepy.cursor and iteration\n",
    "        for page in tweepy.Cursor(self.api.user_timeline, screen_name=username, count=20, include_rts=False,tweet_mode='extended', since = '2020-05-01').pages(5):\n",
    "\n",
    "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
    "            for status in page:\n",
    "                \n",
    "                new_entry = []\n",
    "                status = status._json\n",
    "                \n",
    "                #if this tweet is a retweet update retweet count\n",
    "                if status['created_at'] in df['created_at'].values:\n",
    "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                    #\n",
    "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
    "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
    "                    if cond1 or cond2:\n",
    "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "\n",
    "                #calculate sentiment\n",
    "                filtered_tweet = self.clean_tweets(status['full_text'])\n",
    "                blob = TextBlob(filtered_tweet)\n",
    "                Sentiment = blob.sentiment     \n",
    "                polarity = Sentiment.polarity\n",
    "                subjectivity = Sentiment.subjectivity\n",
    "\n",
    "                new_entry += [status['id'], status['created_at'],\n",
    "                              status['source'], status['full_text'], filtered_tweet, \n",
    "                              Sentiment,polarity,subjectivity, status['lang'],\n",
    "                              status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "                new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "                try:\n",
    "                    is_sensitive = status['possibly_sensitive']\n",
    "                except KeyError:\n",
    "                    is_sensitive = None\n",
    "\n",
    "                new_entry.append(is_sensitive)\n",
    "\n",
    "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "                new_entry.append(hashtags) #append the hashtags\n",
    "\n",
    "                #\n",
    "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "                new_entry.append(mentions) #append the user mentions\n",
    "\n",
    "                try:\n",
    "                    xyz = status['place']['bounding_box']['coordinates']\n",
    "                    coordinates = [coord for loc in xyz for coord in loc]\n",
    "                except TypeError:\n",
    "                    coordinates = None\n",
    "                #\n",
    "                new_entry.append(coordinates)\n",
    "\n",
    "                try:\n",
    "                    location = status['user']['location']\n",
    "                except TypeError:\n",
    "                    location = ''\n",
    "                #\n",
    "                new_entry.append(location)\n",
    "\n",
    "                #now append a row to the dataframe\n",
    "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
    "                df = df.append(single_tweet_df, ignore_index=True)\n",
    "\n",
    "        #\n",
    "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
    "        df = df.sort_values('timestamp').set_index('timestamp')\n",
    "        df = df.drop('id',axis=1)\n",
    "\n",
    "        if not csvfile is None:\n",
    "            #save it to file\n",
    "            df.to_csv(csvfile, index=True,mode='a', encoding=\"utf-8\")\n",
    "            \n",
    "\n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EswatiniGovern1\n",
      "MalawiGovt\n",
      "hagegeingob\n",
      "FinanceSC\n",
      "PresidencyZA\n",
      "mohzambia\n",
      "edmnangagwa\n",
      "MinSantedj\n",
      "hawelti\n",
      "StateHouseKenya\n",
      "PaulKagame\n",
      "M_Farmaajo\n",
      "SouthSudanGov\n",
      "SudanPMHamdok\n",
      "TZSpokesperson\n",
      "KagutaMuseveni\n",
      "angola_Mirex\n",
      "willynyamitwe\n",
      "Cherif_MZ\n",
      "Presidence_RDC\n",
      "PresidentABO\n",
      "PresidenceBenin\n",
      "rochkaborepf\n",
      "PresidenciaCV\n",
      "AOuattara_PRCI\n",
      "Presidency_GMB\n",
      "NAkufoAddo\n",
      "President_GN\n",
      "USEmbalo\n",
      "PresidenceMali\n",
      "CheikhGhazouani\n",
      "IssoufouMhm\n",
      "MBuhari\n",
      "Macky_Sall\n",
      "PresidentBio\n",
      "MSPS_Togo\n"
     ]
    }
   ],
   "source": [
    "for username in leader_usernames:\n",
    "    try:\n",
    "        ts = tweetsearch()\n",
    "        df = ts.get_tweets_again(username, csvfile='data/Leaders_Tweets_User.csv')\n",
    "        #Helps keep track of usernames\n",
    "        print(username)\n",
    "    except tweepy.TweepError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EswatiniGovern1\n",
      "MalawiGovt\n",
      "hagegeingob\n",
      "FinanceSC\n",
      "PresidencyZA\n",
      "mohzambia\n",
      "edmnangagwa\n",
      "MinSantedj\n",
      "hawelti\n",
      "StateHouseKenya\n",
      "PaulKagame\n",
      "M_Farmaajo\n",
      "SouthSudanGov\n",
      "SudanPMHamdok\n",
      "TZSpokesperson\n",
      "KagutaMuseveni\n",
      "angola_Mirex\n",
      "willynyamitwe\n",
      "Cherif_MZ\n",
      "Presidence_RDC\n",
      "PresidentABO\n",
      "PresidenceBenin\n",
      "rochkaborepf\n",
      "PresidenciaCV\n",
      "AOuattara_PRCI\n",
      "Presidency_GMB\n",
      "NAkufoAddo\n",
      "President_GN\n",
      "USEmbalo\n",
      "PresidenceMali\n",
      "CheikhGhazouani\n",
      "IssoufouMhm\n",
      "MBuhari\n",
      "Macky_Sall\n",
      "PresidentBio\n",
      "MSPS_Togo\n"
     ]
    }
   ],
   "source": [
    "for username in leader_usernames:\n",
    "    try:\n",
    "        ts = tweetsearch()\n",
    "        df = ts.get_tweets(username, csvfile='data/Leaders_Tweets.csv')\n",
    "        #Helps keep track of usernames\n",
    "        print(username)\n",
    "    except tweepy.TweepError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gettleman\n",
      "a24media\n",
      "andiMakinana\n",
      "AfricaCheck\n",
      "JamesCopnall\n",
      "oafrica\n",
      "PatrickNgowi\n",
      "StateAfrica\n",
      "Moadow\n",
      "BrendanSAfrica\n",
      "CityTshwane\n",
      "VISI_Mag\n",
      "ThisIsAfricaTIA\n",
      "sarzss\n",
      "TheEIU_Africa\n",
      "InvestInAfrica\n",
      "malonebarry\n",
      "artsouthafrica\n",
      "KahnMorbee\n",
      "JamalMOsman\n",
      "iamsuede\n",
      "mikestopforth\n",
      "equal_education\n",
      "t_mcconnell\n",
      "forbeesta\n",
      "jaxpanik\n",
      "thisisafrica\n",
      "audisouthafrica\n",
      "ONEinAfrica\n",
      "Hamza_Africa\n",
      "africatechie\n",
      "cx73\n",
      "ayittey\n",
      "MercedesBenz_SA\n",
      "africagathering\n",
      "okayafrica\n",
      "mary_harper\n",
      "savetherhino\n",
      "pressfreedom \n",
      "TechCentral\n",
      "GautengProvince\n",
      "Aynte\n",
      "daniel_howden\n",
      "rangerdiaries\n",
      "TheStar_news\n",
      "schneiderhome\n",
      "theafricareport\n",
      "CityofJoburgZA\n",
      "ThinkAfricaFeed\n",
      "AfricaGoodNews\n",
      "willintune\n",
      "cnbcafrica\n",
      "MadeItInAfrica\n",
      "AfricaResearch\n",
      "FoodBlogCT\n",
      "MbuyiseniNdlozi\n",
      "africaprogress\n",
      "IFCAfrica\n",
      "HenleyAfrica\n",
      "geoffreyyork\n",
      "Entrepreneur_SA\n",
      "forbesafrica\n",
      "IECSouthAfrica\n",
      "art2gee\n",
      "JendayiFrazer\n",
      "PeterGreste\n",
      "NDOCKenya\n",
      "Mo_IbrahimFdn\n",
      "ParliamentofRSA\n",
      "SandtonCity\n",
      "_AfricanUnion\n",
      "gertjohan\n",
      "SmithInAfrica\n",
      "hartleyr\n",
      "liveamp\n",
      "SamsungSA\n",
      "BobSkinstad\n",
      "Camfed\n",
      "BBCAndrewH\n",
      "euphonik\n",
      "UlrichJvV\n",
      "OfficialCSA\n",
      "MTVbaseAfrica\n",
      "Computicket\n",
      "loyisogola\n",
      "5FM\n",
      "mailandguardian\n",
      "helenzille\n",
      "Julius_S_Malema\n",
      "News24\n",
      "SAPresident\n",
      "GarethCliff\n",
      "Trevornoah\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for username in influencer_usernames:\n",
    "    try:\n",
    "        ts = tweetsearch()\n",
    "        df = ts.get_tweets_again(username, csvfile='data/Influencers_Tweets_User.csv')\n",
    "        #Helps keep track of usernames\n",
    "        print(username)\n",
    "    except tweepy.TweepError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gettleman\n",
      "a24media\n",
      "andiMakinana\n",
      "AfricaCheck\n",
      "JamesCopnall\n",
      "oafrica\n",
      "PatrickNgowi\n",
      "StateAfrica\n",
      "Moadow\n",
      "BrendanSAfrica\n",
      "CityTshwane\n",
      "VISI_Mag\n",
      "andBeyondSafari\n",
      "ThisIsAfricaTIA\n",
      "sarzss\n",
      "TheEIU_Africa\n",
      "InvestInAfrica\n",
      "malonebarry\n",
      "artsouthafrica\n",
      "KahnMorbee\n",
      "JamalMOsman\n",
      "iamsuede\n",
      "mikestopforth\n",
      "equal_education\n",
      "t_mcconnell\n",
      "forbeesta\n",
      "hurricanevaness\n",
      "BBCKarenAllen\n",
      "jaxpanik\n",
      "thisisafrica\n",
      "audisouthafrica\n",
      "ONEinAfrica\n",
      "Hamza_Africa\n",
      "drewfhinshaw\n",
      "africatechie\n",
      "cx73\n",
      "ayittey\n",
      "MercedesBenz_SA\n",
      "africagathering\n",
      "okayafrica\n",
      "mary_harper\n",
      "savetherhino\n",
      "pressfreedom \n",
      "TechCentral\n",
      "GautengProvince\n",
      "Aynte\n",
      "daniel_howden\n",
      "rangerdiaries\n",
      "TheStar_news\n",
      "schneiderhome\n",
      "Afrinnovator\n",
      "theafricareport\n",
      "CityofJoburgZA\n",
      "ThinkAfricaFeed\n",
      "AfricaGoodNews\n",
      "willintune\n",
      "cnbcafrica\n",
      "MadeItInAfrica\n",
      "AfricaResearch\n",
      "FoodBlogCT\n",
      "MbuyiseniNdlozi\n",
      "africaprogress\n",
      "IFCAfrica\n",
      "HenleyAfrica\n",
      "The_New_Age\n",
      "geoffreyyork\n",
      "Entrepreneur_SA\n",
      "forbesafrica\n",
      "IECSouthAfrica\n",
      "art2gee\n",
      "JendayiFrazer\n",
      "PeterGreste\n",
      "NDOCKenya\n",
      "Mo_IbrahimFdn\n",
      "ParliamentofRSA\n",
      "SandtonCity\n",
      "_AfricanUnion\n",
      "gertjohan\n",
      "SmithInAfrica\n",
      "hartleyr\n",
      "liveamp\n",
      "SamsungSA\n",
      "BobSkinstad\n",
      "Camfed\n",
      "BBCAndrewH\n",
      "euphonik\n",
      "UlrichJvV\n",
      "702JohnRobbie\n",
      "OfficialCSA\n",
      "MTVbaseAfrica\n",
      "Computicket\n",
      "loyisogola\n",
      "5FM\n",
      "mailandguardian\n",
      "helenzille\n",
      "Julius_S_Malema\n",
      "News24\n",
      "SAPresident\n",
      "GarethCliff\n",
      "Trevornoah\n"
     ]
    }
   ],
   "source": [
    "for username in influencer_usernames:\n",
    "    try:\n",
    "        ts = tweetsearch()\n",
    "        df = ts.get_tweets(username, csvfile='data/Influencers_Tweets.csv')\n",
    "        #Helps keep track of usernames\n",
    "        print(username)\n",
    "    except tweepy.TweepError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to stream it so my information is always updated\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def __init__(self,fhandle, stop_at = 100):\n",
    "        self.tweet_counter = 0\n",
    "        self.stop_at = stop_at\n",
    "        self.fhandle = fhandle\n",
    "         \n",
    "        \n",
    "    def on_data(self, data):\n",
    "        self.fhandle.write(data)\n",
    "        \n",
    "        #stop if enough tweets are obtained\n",
    "        self.tweet_counter += 1   \n",
    "        if self.tweet_counter < self.stop_at:        \n",
    "            return True\n",
    "        else:\n",
    "            print('Max number of tweets reached: #tweets = ' + str(self.tweet_counter))\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "\n",
    "def stream_tweet_data(filename='data/tweets_data.csv',\n",
    "                      keywords=username,\n",
    "                      is_async=False):\n",
    "    # tweet topics to use as a filter. The tweets downloaded\n",
    "    # will have one of the topics in their text or hashtag \n",
    "\n",
    "    print('saving data to file: ',filename)\n",
    "\n",
    "    #print the tweet topics \n",
    "    print('Tweet Users are: ',keywords)\n",
    "\n",
    "    #Variables that contains the user credentials to access Twitter API \n",
    "    consumer_key = os.environ.get('TWITTER_API_KEY')\n",
    "    consumer_secret = os.environ.get('TWITTER_API_SECRET')\n",
    "    access_token = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "    access_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "    \n",
    "\n",
    "    #open file \n",
    "    fhandle=open(filename,'a+')\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener(fhandle)\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: first argument to this code\n",
    "    stream.filter(track=keywords,is_async=is_async)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Leaders_Tweets = 'data/Streamed_Leaders_Tweets.json'\n",
    "Influencers_Tweets = 'data/Streamed_Influencers_Tweets.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  EswatiniGovern1\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  MalawiGovt\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  hagegeingob\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  FinanceSC\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  PresidencyZA\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  mohzambia\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  edmnangagwa\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  MinSantedj\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  hawelti\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  StateHouseKenya\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  PaulKagame\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  M_Farmaajo\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  SouthSudanGov\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  SudanPMHamdok\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  TZSpokesperson\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  KagutaMuseveni\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  angola_Mirex\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  willynyamitwe\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  Cherif_MZ\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  Presidence_RDC\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  PresidentABO\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  PresidenceBenin\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  rochkaborepf\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  PresidenciaCV\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  AOuattara_PRCI\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  Presidency_GMB\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  NAkufoAddo\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  President_GN\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  USEmbalo\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  PresidenceMali\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  CheikhGhazouani\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  IssoufouMhm\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  MBuhari\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  Macky_Sall\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  PresidentBio\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Leaders_Tweets.json\n",
      "Tweet Users are:  MSPS_Togo\n",
      "Max number of tweets reached: #tweets = 100\n"
     ]
    }
   ],
   "source": [
    "#warning, May take longer than expected\n",
    "for username in leader_usernames:\n",
    "    try:\n",
    "        stream_tweet_data(filename=Leaders_Tweets, keywords = username)    \n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  gettleman\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  a24media\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  andiMakinana\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  AfricaCheck\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  JamesCopnall\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  oafrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  PatrickNgowi\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  StateAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Moadow\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  BrendanSAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  CityTshwane\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  VISI_Mag\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  andBeyondSafari\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  ThisIsAfricaTIA\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  sarzss\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  TheEIU_Africa\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  InvestInAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  malonebarry\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  artsouthafrica\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  KahnMorbee\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  JamalMOsman\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  iamsuede\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  mikestopforth\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  equal_education\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  t_mcconnell\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  forbeesta\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  hurricanevaness\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  BBCKarenAllen\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  jaxpanik\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  thisisafrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  audisouthafrica\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  ONEinAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Hamza_Africa\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  drewfhinshaw\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  africatechie\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  cx73\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  ayittey\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  MercedesBenz_SA\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  africagathering\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  okayafrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  mary_harper\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  savetherhino\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  pressfreedom \n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  TechCentral\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  GautengProvince\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Aynte\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  daniel_howden\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  rangerdiaries\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  TheStar_news\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  schneiderhome\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Afrinnovator\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  theafricareport\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  CityofJoburgZA\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  ThinkAfricaFeed\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  AfricaGoodNews\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  willintune\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  cnbcafrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  MadeItInAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  AfricaResearch\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  FoodBlogCT\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Users are:  MbuyiseniNdlozi\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  africaprogress\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  IFCAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  HenleyAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  The_New_Age\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  geoffreyyork\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Entrepreneur_SA\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  forbesafrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  IECSouthAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  art2gee\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  JendayiFrazer\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  PeterGreste\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  NDOCKenya\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Mo_IbrahimFdn\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  ParliamentofRSA\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  SandtonCity\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  _AfricanUnion\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  gertjohan\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  SmithInAfrica\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  hartleyr\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  liveamp\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  SamsungSA\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  BobSkinstad\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Camfed\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  BBCAndrewH\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  euphonik\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  UlrichJvV\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  702JohnRobbie\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  OfficialCSA\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  MTVbaseAfrica\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Computicket\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  loyisogola\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  5FM\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  mailandguardian\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  helenzille\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Julius_S_Malema\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  News24\n",
      "420\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  SAPresident\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  GarethCliff\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  Trevornoah\n",
      "Max number of tweets reached: #tweets = 100\n",
      "saving data to file:  data/Streamed_Influencers_Tweets.json\n",
      "Tweet Users are:  None\n",
      "406\n",
      "406\n",
      "420\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n",
      "406\n"
     ]
    }
   ],
   "source": [
    "for username in influencer_usernames:\n",
    "    try:\n",
    "        stream_tweet_data(filename=Influencers_Tweets, keywords = username)    \n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
